---
title: Conversation reviews
---

Conversation Review is where you check **what actually happened** in a call or chat.

This page shows transcripts, basic system decisions, and supporting metadata so you can confirm whether the agent behaved as expected.

If you cannot explain what happened in a conversation using this page, the agent is not ready to move forward.

## What Conversation Review is for (Level 1)

At this stage, use Conversation Review to answer simple but critical questions:

> Did the agent understand the user?
>
> Did it answer from the right Knowledge Base topic?
>
> Did it escalate or hand off when expected?
>
> Did anything obviously go wrong?

You are not tuning behaviour yet. You are confirming that the basics work.

## Starting in Conversations

Begin on the **Conversations** page.

This table lists all interactions across Chat and Call.

Focus on these columns:
- **Status**: Live or Ended
- **Summary**: High-level intent label
- **Start time**: Helps align with test runs
- **Contact**: Phone number or user identifier
- **Duration**: Longer calls often signal confusion
- **PolyScore** (if enabled): Quick quality indicator

Use **Edit table** to add helpful columns:
- Environment
- Variant
- Handoff reason
- Function call

This makes basic issues easier to spot without opening every conversation.

## Filtering during testing

Use **Filter conversations** to narrow your view.

Common Level 1 filters:
- Environment = Sandbox
- Start date = Today
- Duration > 60 seconds
- Status = Ended

> Example:
> Filter to *Sandbox + Duration > 90s* to find conversations where the agent struggled.

## Opening a conversation

Clicking a row opens **Conversation Review**.

You will see three main areas:
1. Transcript (centre)
2. Conversation details (left)
3. Diagnostic toggles (right)

At Level 1, focus primarily on the **transcript** and **matched topics**.

## Reading the transcript

The transcript shows the conversation turn by turn.

For each turn, check:
- What the user said (ASR text)
- How the agent responded
- Which KB topic was matched (shown beneath the turn)

> Example:
> User: “late checkout”
>
> Matched topic: `late_checkout_policy`

This is the expected outcome. If the wrong topic appears, the issue is almost always in KB naming or sample questions.

## Matched Knowledge Base topics

Matched topics are your primary signal at this level.

Use them to confirm:
- The correct topic was selected
- Similar topics are not competing
- The topic name reflects the intent clearly

> Example:
> The agent answers about billing, but the matched topic is `general_help`.
>
> This indicates a KB design problem, not a phrasing issue.

## Conversation details (left panel)

The left panel confirms context.

Check:
- Environment (Sandbox vs Live)
- Channel (Chat or Call)
- Language and locale
- Duration
- Variant (if applicable)

This helps ensure you are reviewing the correct test and not a different version of the agent.

## Basic diagnosis toggles

At Level 1, you only need a small subset of diagnosis tools.

### Topic citations
Use this to confirm which KB topic was used for each response.

If the agent answers correctly but cites the wrong topic, fix the KB before proceeding.

### Function calls
Use this to confirm:
- SMS was only sent after consent
- Handoff occurred when expected

You do not need to inspect parameters in detail yet — only whether the action happened at all.

## Audio playback (Calls only)

For call channels, use audio playback to:
- Confirm ASR quality
- Hear how long responses feel when spoken
- Check whether users interrupt the agent

Split audio is especially useful for hearing the caller clearly.

## Annotations

Annotations let you flag issues directly from the review page.

At Level 1, use:
- **Wrong transcription** → ASR problem
- **Missing topic** → KB gap

Annotations help you build a clear list of fixes instead of relying on memory.

## What "good" looks like at Level 1

A successful review session should allow you to say:

- The agent selected the correct KB topic
- The response made sense for the question
- Escalation behaviour was reasonable
- Any problems are clearly attributable to KB or ASR

If you cannot clearly state *what to fix*, stop and simplify before adding complexity.

## Final check before Level 2

Before moving on:

- You can explain each response in plain terms
- You can identify whether an issue is KB or ASR
- You are confident the agent behaves consistently in Sandbox

Level 2 will focus on **control, variation, and optimisation**.
Conversation Review is the foundation that makes those steps possible.

<Card>
  <label style={{ display: "flex", gap: "0.75rem", alignItems: "center" }}>
    <input type="checkbox" />
    <div>
      <strong>Progress</strong><br />
      5 of 5 lessons complete
    </div>
  </label>
</Card>