---
title: "Safety"
description: "Ensure your agent is dealing with risky conversations effectively."
---

The safety dashboard offers a comprehensive view of safety-related metrics, helping managers monitor flagged utterances, evaluate risk levels, and ensure compliance with safety and brand standards. It is an essential tool for assessing the agent’s ability to manage potentially harmful conversations.

![safety-dashboard](/images/safety-dashboard.png)

## Metrics

- **Caller utterance risk level**: Assesses overall risk distribution and evaluates the agent's effectiveness in mitigating unsafe conversations.
- **Total calls**: Displays the total number of calls handled during the selected period.
- **Number of calls managed for risk**: Tracks the frequency of triggered safety filters, helping to evaluate filter performance.
- **Percentage of calls managed for risk**: Indicates the proportion of flagged calls relative to total calls.
- **Distribution of flagged calls**: Visualizes flagged call activity over time to identify trends.
- **Distribution count of flagged calls**: Provides granular insights into the frequency of flagged calls, highlighting peak periods.
- **Caller utterance category distribution**:
  - Categorized into **hate speech**, **self-harm**, **sexual content**, and **violence**.
  - Visualized with color-coded breakdowns for easier interpretation.

## Editing safety filters

To edit your filters, go to **Settings** on the sidebar.

![safety-dashboard](/images/safety-filters.png)

PolyAI content safety filters are designed to monitor and manage potentially harmful outputs from AI models and harmful inputs from users. These filters use a
 combination of in-house models and third-party APIs to ensure your assistant responds appropriately and keeps it safe from any malicious users.

### How do safety filters work?

Content safety filters run on both user inputs and the generated outputs. The filters are designed to detect and prevent the passing of harmful content
from the user to the assistant and vice versa. The filtering system specifically targets various categories of potentially harmful content, ensuring that both user
inputs and AI outputs adhere to our safety standards.

### Content filtering categories and severity levels

The content filtering system covers four primary categories of harmful content:

- **Hate**
- **Sexual**
- **Violence**
- **Self-harm**

### Categories

| **Category**       | **Description** |
|--------------------|---------------|
| **Hate** | Hate and fairness-related harms refer to any content that attacks or uses discriminatory language with reference to a person or identity group based on differentiating attributes. This includes, but is not limited to: race, ethnicity, nationality, gender identity and expression, sexual orientation, religion, personal appearance and body size, disability status, harassment, and bullying. |
| **Sexual** | Sexual describes language related to anatomical organs and genitals, romantic relationships and sexual acts, acts portrayed in erotic or affectionate terms, including those portrayed as an assault or a forced sexual violent act against one’s will. This includes but is not limited to: vulgar content, prostitution, nudity and pornography, abuse, child exploitation, child abuse, and child grooming. |
| **Violence** | Violence describes language related to physical actions intended to hurt, injure, damage, or kill someone or something; describes weapons, guns, and related entities. This includes, but isn't limited to: weapons, bullying and intimidation, terrorist and violent extremism, and stalking. |
| **Self-Harm** | Self-harm describes language related to physical actions intended to purposely hurt, injure, damage one’s body, or kill oneself. This includes, but isn't limited to: eating disorders, self-injury, and suicide-related discussions. |

Each of these categories is monitored across four severity levels:

- **Safe**: Content at this level is labeled but not filtered.
- **Lenient**: More content is allowed to pass to the AI model.
- **Medium**: Medium amount of content is allowed to pass to the AI model.
- **Strict**: Strict rules on what content can pass to the AI model.

Content detected at **lenient, medium, and strict severity levels** is subject to filtering, with the degree of action taken varying based on the severity.

### Additional filtering

Beyond the primary categories, the content filtering system includes **jailbreak risk identification**, which helps prevent users from
attempting to bypass safety mechanisms.

### Language support

The content filtering models for **hate, sexual, violence, and self-harm** categories have been specifically trained and tested in several languages, including:

- English
- German
- Japanese
- Spanish
- French
- Italian
- Portuguese
- Chinese

While the service supports many other languages, the quality of detection and filtering may vary, so thoroughly test to ensure
filters are effective in your specific application.

## Best practices

- **Custom testing**: Always conduct thorough testing to ensure the content filtering system works effectively for your application's specific needs.
- **Balanced approach**: Strive to balance between filtering harmful content and maintaining usability to prevent over-restriction that could hinder legitimate activities.




