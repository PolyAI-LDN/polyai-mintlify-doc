---
title: "Overview"
description: "Run saved conversations to check for behavioral regressions in assistant responses."
---

**Test Suite** lets builders save real conversations and re-run them to check whether the assistant continues to behave as expected. It's designed for teams working with generative AI, where responses may vary — making repeatable testing essential.

You can create test cases directly from the chat panel or conversation review, then run them later against **draft** or **sandbox** versions to verify that key behaviors still occur.

![test-suite](/images/test-suite.png)

## Overview

Test Suite helps you validate that core assistant behavior doesn't drift. Each test case includes:

- A real conversation transcript (user messages + assistant replies)
- The functions called during the conversation
- A built-in comparison that flags mismatches in behavior on re-run

Builders can:
- Save test cases from chat or review
- Group them into test runs
- View structured results in a visual dashboard

## How it works

<Steps>

<Step title="Save a test case from chat or conversation review">

<Frame><img width="425" src="/images/create-test-case.png" /></Frame>

Click the **Create test** button (test tube icon) from the chat panel or conversation review page.

You'll be prompted to name your test case and save it.

![test-case-name](/images/test-case-name.png)
</Step>

<Step title="Test case contents and matching logic">

![test-suite-detail](/images/test-suite.png)

Each test case includes the original conversation and the functions triggered during it. When you run it again, the system checks whether the same functions are called in the same places.

This comparison is what determines the pass/fail result.

<Tip>You don't need to configure anything manually — success is based on reproducing original behavior.</Tip>
</Step>

<Step title="Run tests — individually or in groups">

You can run a test case directly from its detail page, or select multiple saved test cases and run them as a group.

![test-case-run-from-menu](/images/test-case-run-from-menu.png)

Give your run a name and choose which version of the assistant to test (Draft or Sandbox).
</Step>

<Step title="View run results and investigate regressions">

Once the run finishes, results appear under the **Test Runs** tab.

![test-runs-completed](/images/test-runs-completed.png)

You'll see overall success rates, agent versions, and timestamps. You can click in to see which cases passed or failed — and compare the conversation behavior directly.

</Step>

</Steps>

Use Test Suite to safeguard against regressions, validate flow updates, and ensure your assistant continues to respond the way you expect. For help or feedback, contact [platform-support@poly-ai.com](mailto:platform-support@poly-ai.com).